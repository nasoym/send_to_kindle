<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>When to use the Pushgateway</title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<div id="header">
<h1 class="title">When to use the Pushgateway</h1>
</div>
<h1 id="when-to-use-the-pushgateway">When to use the Pushgateway</h1>
<p>The Pushgateway is an intermediary service which allows you to push metrics from jobs which cannot be scraped. For details, see <a href="/docs/instrumenting/pushing/">Pushing metrics</a>.</p>
<h2 id="should-i-be-using-the-pushgateway">Should I be using the Pushgateway?</h2>
<p><strong>We only recommend using the Pushgateway in certain limited cases.</strong> There are several pitfalls when blindly using the Pushgateway instead of Prometheus's usual pull model for general metrics collection:</p>
<ul>
<li>When monitoring multiple instances through a single Pushgateway, the Pushgateway becomes both a single point of failure and a potential bottleneck.</li>
<li>You lose Prometheus's automatic instance health monitoring via the <code>up</code> metric (generated on every scrape).</li>
<li>The Pushgateway never forgets series pushed to it and will expose them to Prometheus forever unless those series are manually deleted via the Pushgateway's API.</li>
</ul>
<p>The latter point is especially relevant when multiple instances of a job differentiate their metrics in the Pushgateway via an <code>instance</code> label or similar. Metrics for an instance will then remain in the Pushgateway even if the originating instance is renamed or removed. This is because the lifecycle of the Pushgateway as a metrics cache is fundamentally separate from the lifecycle of the processes that push metrics to it. Contrast this to Prometheus's usual pull-style monitoring: when an instance disappears (intentional or not), its metrics will automatically disappear along with it. When using the Pushgateway, this is not the case, and you would now have to delete any stale metrics manually or automate this lifecycle synchronization yourself.</p>
<p><strong>Usually, the only valid use case for the Pushgateway is for capturing the outcome of a service-level batch job</strong>. A &quot;service-level&quot; batch job is one which is not semantically related to a specific machine or job instance (for example, a batch job that deletes a number of users for an entire service). Such a job's metrics should not include a machine or instance label to decouple the lifecycle of specific machines or instances from the pushed metrics. This decreases the burden for managing stale metrics in the Pushgateway. See also the <a href="/docs/practices/instrumentation/#batch-jobs">best practices for monitoring batch jobs</a>.</p>
<h2 id="alternative-strategies">Alternative strategies</h2>
<p>If an inbound firewall or NAT is preventing you from pulling metrics from targets, consider moving the Prometheus server behind the network barrier as well. We generally recommend running Prometheus servers on the same network as the monitored instances.</p>
<p>For batch jobs that are related to a machine (such as automatic security update cronjobs or configuration management client runs), expose the resulting metrics using the <a href="https://github.com/prometheus/node_exporter">Node Exporter's</a> textfile module instead of the Pushgateway.</p>
</body>
</html>
